taboo = {'JAEILC010000038', 
        'GECV01031551', 
        'AE006468', 
        'CP015418', 
        'CP000031', 
        'CP000830', 
        'CP001312', 
        'CP001357', 
        'BX897699', 
        'AUXO01792325', 
        'OQ735257', 
        'QQ198719', 
        'QQ198717', 
        'QQ198718',
        'OQ7219011',
        'QNQ73380',
        'QMP84020',
        'QJT73696',
        'QJT73701',
        'QJT73698',
        'SRR2729873', 
        'AUXO017923253', 
        'K03573', 
        'C978956',
        'AF181082',
        'D01221',
        'MT29357',
        'N5326222'} # known invalid accessions
known_typos = {'HQHQ847905':'HQ847905', 
               'EU7257772':'EU725772', 
               'LCM141331':'KX884774', 
               'SKR870013':'KR870013', 
               'AF4362513':'AF362513', 
               'JX4782635':'JX478263',
               'NC010308':'NC_010308',
               'NC021720':'NC_021720',
               'NC035620':'NC_035620',
               'NC028131':'NC_028131',
               'NC027432':'NC_027432',
               'NC012805':'NC_012805',
               'NC012127':'NC_012127'
               } # known typos due to manual annotation, we salvage those by replacing with a list containing the polished accession

import pandas as pd
import numpy as np
import subprocess
import hashlib
import argparse
import logging
import sqlite3
import sys
import os
import re

def parse_arguments():
    parser = argparse.ArgumentParser(description='Access any version of the Virus Metadata Resource and store the data into a database.')
    parser.add_argument('-d', '--database', type=str, required=True, 
                        help='Name of database to read from/add sequences to. Will create if it does not exist, though having it saves a lot of time.')
    parser.add_argument('-o', '--output', type=str, required=True, 
                        help='Name of working directory to be created if not existing')
    parser.add_argument('-u', '--url', default='https://ictv.global/vmr/current', 
                        help='URL to the desired VMR version. Navigate to the ICTV website, under Virus Metadata Resource right-click on any spreadsheet and copy the link.')
    args = parser.parse_args()
    return args

logging.basicConfig(format='%(asctime)s - %(funcName)s:%(lineno)d [%(levelname)s] %(message)s', datefmt='%d-%b-%y %H:%M:%S', level=logging.INFO)

def DownloadInternetFile(url, output):
    from subprocess import Popen, PIPE
    def run_cmd(cmd):
        process = Popen(cmd, stdout=PIPE, stderr=PIPE)
        stdout, stderr = process.communicate()
    logging.info(f'Downloading internet file {url}')
    cmd = ["wget", url, "--no-check-certificate", "-O", output]
    run_cmd(cmd)
    if not os.path.isfile(output):
        return 1
    return 0

def CreateDirectory(directory_path):
    try:
        os.mkdir(directory_path)
    except FileExistsError:
        pass # Ignore if already present
    except: # Anything else produces error, return status 1
        logging.error(f'Cannot create directory {directory_path}')
        return 1
    logging.info(f'Successfully created/updated directory at {directory_path}')
    return 0

def parse_raw_accession_string(s):
    if pd.isnull(s):
        return set()
    normalized_string = re.sub(r'[ \t\n\r\f\v:;,+-]+', '@', s) ## After much search we settled for this
    parts = set(part for part in normalized_string.split('@') 
                if len(part) > 5 and not re.search(r'[a-z]', part) and part and part[0].isalpha() and not 'DNA' in part)
    return sorted(parts)

def itr(a, b):
    for i in range(0, len(a), b): yield  a[i:i+b]

def eval_accessions(acc_list):
    cmd = f"efetch -db nuccore -id {','.join(acc_list)} -format acc"
    result = subprocess.check_output(cmd, shell = True, executable = "/bin/bash", stderr = subprocess.STDOUT).decode()
    result = result.split('\n')
    matching = set(x for x in acc_list for y in result if f'{x}.' in y)
    if len(matching) != len(acc_list):
        logging.info(f'Detected invalid accessions {set.symmetric_difference(matching, set(iter))}')
    return matching

def download_by_accessions(acc_list):
    cmd = f"efetch -db nuccore -id {','.join(acc_list)} -format fasta"
    result = subprocess.check_output(cmd, shell = True, executable = "/bin/bash", stderr = subprocess.STDOUT).decode()
    if not result.startswith('>'):
        logging.warning('There has been a problem collecting one or more FASTA sequences. Bouncing this batch back')
        logging.warning(f'Batch: {acc_list}')
        return None
    header = "Unknown"
    data = {}
    result = result.split('\n')
    for line in result:
        if not line: continue
        if line.startswith('>'):
            header = line.rsplit('.')[0].split('>')[1]
            if header in acc_list:
                data[header] = f'{line}\n'
        else:
            data[header] += line
    return data

#########################
## Quite procedural
#########################
args = parse_arguments()

args.vmr = os.path.join(args.output, 'VMR')
args.vmr_spreadsheet = os.path.join(args.vmr, 'vmr_spreadsheet.xlsx')
args.valid_accessions = os.path.join(args.output, 'valid_accessions.txt')
args.taxonomy_table = os.path.join(args.output, 'taxonomy_table.csv')

if CreateDirectory(args.output) == 1:
    sys.exit(1)

if CreateDirectory(args.vmr) == 1:
    sys.exit(1)

if DownloadInternetFile(args.url, args.vmr_spreadsheet) == 1:
    sys.exit(1)

## First collapse all genbank accessions
xlsx = pd.read_excel(args.vmr_spreadsheet)
cleanup = set()
for record in xlsx['Virus GENBANK accession']:
    acc_set = parse_raw_accession_string(record)
    if len(acc_set) == 0: continue
    for accession in acc_set:
        if accession in taboo: continue
        typo = known_typos.get(accession, None)
        if typo: accession = typo
        cleanup.add(accession)
cleanup = list(cleanup)

## (1) don't try downloading the FASTA right away, instead, retrieve the accession identifiers
logging.info('Checking valid accessions')
valid_accessions = []
for iter in itr(cleanup, 500):
    matching = eval_accessions(iter)
    valid_accessions += list(matching)


## (2) write the valid accessions to file
logging.info(f'There are {len(valid_accessions)} valid accessions in this instance')
with open(args.valid_accessions, 'w') as hndl:
    for accession in valid_accessions:
        print(accession, file=hndl)

## (3) connect to database
logging.info('Connecting to database using sqlite3')
conn = sqlite3.connect(args.database)
cursor = conn.cursor()
cursor.execute('''
CREATE TABLE IF NOT EXISTS sequences (
    id TEXT PRIMARY KEY,
    sequence TEXT
)
''') ## id is the primary key (the accession)

## (4) Download sequences in batches of 500 and add to database
conn.execute('BEGIN TRANSACTION')

for iter in itr(valid_accessions, 500):
    logging.info(f'Downloading {len(iter)} accessions')
    ## Checking if db contains
    placeholders = ','.join('?' for _ in iter)
    cursor.execute(f'SELECT id FROM sequences WHERE id IN ({placeholders})', iter)
    results = cursor.fetchall()
    found_ids = {row[0] for row in results}
    remaining = list(set.difference( set(iter), found_ids))
    if len(remaining) != len(iter):
        logging.info(f'Some items were already found in database. {len(remaining)} accessions remaining')
    if len(remaining) == 0: continue
    data = download_by_accessions(remaining)
    try:
        if not data: continue
        cursor.executemany('''
        INSERT INTO sequences (id, sequence) VALUES (?, ?)
        ''', [(k,v) for k,v in data.items()])
        conn.commit()
    except sqlite3.Error as e:
        print(f"An error occurred: {e}")
        conn.rollback()
logging.info('Closing connection')
cursor.close()

logging.info('Creating a taxonomy table')
## Now take care of the taxonomy lineage information
xlsx = xlsx.rename(columns={'Host source':'Host Source'})
xlsx = xlsx.rename(columns={'Exemplar or additional isolate ':'Exemplar or additional isolate', 'Virus  name abbreviation(s)':'Virus name abbreviation(s)'})
xlsx = xlsx.rename(columns={'Exemplar (E) or additional isolate (A)':'Exemplar or additional isolate'})
xlsx = xlsx.rename(columns={'Host source':'Host Source'})
xlsx = xlsx.rename(columns={'Species Sort':'Sort'})
xlsx = xlsx.rename(columns={'Host_Source':'Host Source', 'Species Sort':'Sort'})
xlsx = xlsx.rename(columns={'Isolate sort':'Isolate Sort', 'Species sort':'Sort'})
xlsx = xlsx.rename(columns={'Host/Source':'Host Source', 'Isolate sort':'Isolate Sort', 'Species sort':'Sort'})
xlsx = xlsx.rename(columns={'Host/Source':'Host Source', 'Isolate sort':'Isolate Sort', 'Species sort':'Sort', 'Virus REFSEQ accession 200319':'Virus REFSEQ accession'})
xlsx = xlsx.rename(columns={'Exemplar ':'Exemplar or additional isolate', 'Virus  name abbreviation(s)':'Virus name abbreviation(s)', 'Virus designation':'Virus isolate designation', 'family':'Family', 'genus':'Genus', 'order':'Order', 'sort':'Sort', 'species':'Species', 'subfamily':'Subfamily'})
xlsx = xlsx.rename(columns={'Exemplar ':'Exemplar or additional isolate', 'Virus  name abbreviation(s)':'Virus name abbreviation(s)', 'Virus designation':'Virus isolate designation', 'family':'Family', 'genus':'Genus', 'order':'Order', 'sort':'Sort', 'species':'Species', 'subfamily':'Subfamily'})
xlsx = xlsx.rename(columns={'Exemplar ':'Exemplar or additional isolate', 'Virus  name abbreviation(s)':'Virus name abbreviation(s)', 'Virus designation':'Virus isolate designation', 'family':'Family', 'genus':'Genus', 'order':'Order', 'species':'Species', 'subfamily':'Subfamily'})
xlsx = xlsx.rename(columns={'Exemplar ':'Exemplar or additional isolate', 'Virus  name abbreviation(s)':'Virus name abbreviation(s)', 'Virus designation':'Virus isolate designation', 'family':'Family', 'genus':'Genus', 'order':'Order', 'sort':'Sort', 'species':'Species', 'subfamily':'Subfamily'})
xlsx = xlsx.rename(columns={'Exemplar or Additional ':'Exemplar or additional isolate', 'Virus  name abbreviation(s)':'Virus name abbreviation(s)', 'Virus designation':'Virus isolate designation', 'family':'Family', 'genus':'Genus', 'order':'Order', 'sort':'Sort', 'species':'Species', 'subfamily':'Subfamily'})
required_columns = ['Sort','Isolate Sort','Realm','Subrealm','Kingdom','Subkingdom','Phylum','Subphylum','Class','Subclass','Order','Suborder','Family','Subfamily','Genus','Subgenus','Species','Exemplar or additional isolate','Virus name(s)','Virus name abbreviation(s)','Virus isolate designation','Virus GENBANK accession','Virus REFSEQ accession','Genome coverage','Genome composition','Host Source']

invalid_sort = False
if 'sort' not in xlsx.iloc[:,0].name.lower():
    invalid_sort = True

for column in required_columns:
    if column not in xlsx.columns:
        xlsx[column] = np.nan
        if column=='Isolate Sort':
            xlsx[column] = 1

if xlsx['Sort'].sum() == 0 or invalid_sort:
    logging.warning('"Sort" field not properly formatted. Coercing to np.arange(df.shape[0]), i.e., a progressive integer')
    xlsx['Sort'] = np.arange(xlsx.shape[0]) + 1

virus_identifiers = [str(elem[0])+'_'+str(elem[1]) for elem in zip(xlsx['Sort'],xlsx['Isolate Sort'])]
if len(virus_identifiers) != len(set(virus_identifiers)):
    logging.warning('Attempted merging "Sort" with "Isolate Sort" fields but some values are not unique. Proceeding with adding a progressive integer instead of the "Sort", keeping the "Isolate Sort" intact')
    virus_identifiers = []
    isolate_sorts = list(xlsx['Isolate Sort'])
    value = 0
    for i in range(len(isolate_sorts)):
        if isolate_sorts[i]==1:
            value += 1
            virus_identifiers.append(str(value)+'_'+str(isolate_sorts[i]))
        else:
            virus_identifiers.append(str(value)+'_'+str(isolate_sorts[i]))

xlsx['Sort'] = virus_identifiers
xlsx = xlsx.drop_duplicates(subset=['Sort'], keep='first') # At this point the data set is unique enough in terms of Sort. Remove the redundant rest (if present)

flagged_for_removal = []
for i, row in xlsx.iterrows():
    accessions = parse_raw_accession_string(row['Virus GENBANK accession'])
    if not accessions:
        flagged_for_removal.append(i)
    else:
        eligible_accessions = [accession for accession in accessions if accession in valid_accessions]
        if len(eligible_accessions) == 0:
            flagged_for_removal.append(i)
            continue
        xlsx.loc[(xlsx.index==i), 'Virus GENBANK accession'] = ';'.join(eligible_accessions)
logging.info(f'{len(flagged_for_removal)} viruses were flagged for removal, as no GENBANK identifier was found')
xlsx = xlsx.drop(flagged_for_removal)
xlsx = xlsx[['Sort',
             'Isolate Sort',
             'Realm',
             'Subrealm',
             'Kingdom',
             'Subkingdom',
             'Phylum',
             'Subphylum',
             'Class',
             'Subclass',
             'Order',
             'Suborder',
             'Family',
             'Subfamily',
             'Genus',
             'Subgenus',
             'Species',
             'Exemplar or additional isolate',
             'Virus GENBANK accession',
             'Genome coverage',
             'Genome composition',
             'Host Source']]
logging.info(f"Final number of valid sorts: {xlsx['Sort'].unique().size}")

## Last important step: read sequence length and update the headers information
headers = []
md5s = []
conn = sqlite3.connect(args.database)
cursor = conn.cursor()
logging.info('Reading sequence information from database')
rows = xlsx.iterrows()
for i, row in rows:
    accessions = sorted(row['Virus GENBANK accession'].split(';'))
    placeholders = ','.join('?' for _ in accessions)
    cursor.execute(f'SELECT sequence FROM sequences WHERE id IN ({placeholders})', accessions)
    results = cursor.fetchall()
    sequences = [x[0] for x in results] # split the tuple
    sizes = [len(x.split('\n')[1]) for x in sequences]
    number_of_segments = len(sequences)
    sequence_size = sum(sizes)
    header = f"sort_{row['Sort']}_{number_of_segments}_{sequence_size}"
    print(f'Adding header {header} to the taxonomy table')
    headers.append(header)
    md5 = hashlib.md5(''.join(accessions).encode('utf-8')).hexdigest()
    md5s.append(md5)
logging.info('Closing connection')
cursor.close()

xlsx['header'] = headers
xlsx['md5'] = md5s

xlsx.to_csv(args.taxonomy_table, index=False)
logging.info(f'Wrote taxonomy table at {args.taxonomy_table}')
